{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from matplotlib import pyplot as plt\n",
    "import string\n",
    "import re\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via','de'] \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#created a demo-twitter account to get the api credentials. \n",
    "API_KEY = 'ZCJvGQklUkXjbTvPRZbMMfij6'\n",
    "API_SECRET = 'T31bAxymp6jLOtvM4HGt0uY9u5Z5DZyZ6JCxsoODbehzFpOCgL'\n",
    "\n",
    "ACCESS_TOKEN = '1373167224-SZabBx6OG0uTqwtqR99HPOtzDPVr01aB42eXwqb'\n",
    "ACCESS_TOKEN_SECRET = 'l1Z8gJ68jMdNpBnpNwHP8C28HdKne75olMNVuxiS0'\n",
    "\n",
    "\n",
    "twitter = Twython(API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets gather unique tweets on the hashtag \"Halloween\"\n",
    "query=\"Halloween\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pull tweets from Twitter search API. Get ~ 5000 unique Tweets. \n",
    "\n",
    "tweets                          =   []\n",
    "MAX_ATTEMPTS                    =   50\n",
    "COUNT_OF_TWEETS_TO_BE_FETCHED   =   5000\n",
    "\n",
    "for i in range(0,MAX_ATTEMPTS):\n",
    "#     print i, \"max_attempt\"\n",
    "\n",
    "    if(COUNT_OF_TWEETS_TO_BE_FETCHED < len(tweets)):\n",
    "        break # we got 1000 tweets... !!\n",
    "\n",
    "    if(0 == i):\n",
    "        # Query twitter for data. \n",
    "        results = twitter.search(q=query,count=100)\n",
    "    else:\n",
    "        # After the first call we should have max_id from result of previous call. Pass it in query.\n",
    "        results = twitter.search(q=query,count=100,include_entities='true',max_id=next_max_id)\n",
    "\n",
    "    # STEP 2: Save the returned tweets\n",
    "    for result in results['statuses']:\n",
    "        tweet_text = result['text']\n",
    "        tweets.append(tweet_text)\n",
    "#     print len(tweets), 'length of tweets this page', i    \n",
    "\n",
    "\n",
    "    # STEP 3: Get the next max_id\n",
    "    try:\n",
    "        # Parse the data returned to get max_id to be passed in consequent call.\n",
    "        next_results_url_params  = results['search_metadata']['next_results']\n",
    "        next_max_id = next_results_url_params.split('max_id=')[1].split('&')[0]\n",
    "#         print next_max_id\n",
    "    except:\n",
    "        # No more next pages\n",
    "        break\n",
    "\n",
    "# print len(tweets), \"length of tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre-process tweets. Tokenize, remove punctuations, stopwords\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'halloween', 4582)\n",
      "(u'happy', 467)\n",
      "(u'#halloween', 372)\n",
      "(u'party', 332)\n",
      "(u'barcelona', 278)\n",
      "(u'2015', 263)\n",
      "(u'costume', 247)\n",
      "(u'pesta', 210)\n",
      "(u'picu', 207)\n",
      "(u'kontroversi', 207)\n",
      "(u'amp', 193)\n",
      "(u'atas', 186)\n",
      "(u'candy', 183)\n",
      "(u'everyone', 175)\n",
      "(u\"it's\", 175)\n",
      "(u'getafe', 171)\n",
      "(u'bts', 169)\n",
      "(u'barca', 168)\n",
      "(u'night', 164)\n",
      "(u'like', 163)\n",
      "(u'#halloween', 372)\n",
      "(u'#', 297)\n",
      "(u'#jimin', 39)\n",
      "(u'#fail', 34)\n",
      "(u'#racist', 34)\n",
      "(u'#sorrynotsorry', 33)\n",
      "(u'#jungkook', 31)\n",
      "(u'#suga', 26)\n",
      "(u'#party', 18)\n",
      "(u'#halloween2015', 16)\n",
      "(u'#day6', 16)\n",
      "(u'#pushawardskathniels', 15)\n",
      "(u'#fcbnews', 15)\n",
      "(u'#horror', 14)\n",
      "(u'#ghosts', 14)\n",
      "(u'#costume', 13)\n",
      "(u'#rapmonster', 13)\n",
      "(u'#photography', 11)\n",
      "(u'#trickortreat', 11)\n",
      "(u'#jin', 10)\n"
     ]
    }
   ],
   "source": [
    "#print out most common terms and hashtags after removing stopwords/punctuations. \n",
    "count_all = Counter()\n",
    "count_all_hashtags=Counter()\n",
    "for sentence in tweets:\n",
    "    cleaned_terms = [term for term in preprocess(sentence.lower()) if term not in stop and 32< ord(term[0]) < 128]\n",
    "    cleaned_hashtags = [term for term in preprocess(sentence.lower()) if term.startswith('#')]\n",
    "    count_all.update(cleaned_terms)\n",
    "    count_all_hashtags.update(cleaned_hashtags)\n",
    "\n",
    "\n",
    "for item in count_all.most_common(20):\n",
    "    print item\n",
    "    \n",
    "for item in count_all_hashtags.most_common(20):\n",
    "    print item\n",
    "    \n",
    "distinct_terms=set(cleaned_terms)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to look at correlations between terms (and their related terms) and hashtags. We want to see what are the top terms that often appear with each hashtags so we can calculate a probability of a certain hashtag for a tweet. \n",
    "\n",
    "This idea is simple but a differntiating factor is that i want to use Conceptnet5's huge corpus of English and Mandarin sematic network to help my model understand the associations between terms to predict hashtags with more accuracy instead of just predicting well on the most common terms. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
